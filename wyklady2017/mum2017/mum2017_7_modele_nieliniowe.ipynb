{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Modele nieliniowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 id=tocheading>Spis treści</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set(font_scale=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Demo of unicode support in text and labels.\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.datasets import load_svmlight_file, load_boston, load_breast_cancer\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_boston, load_diabetes, load_linnerud, make_regression\n",
    "from sklearn.datasets import make_friedman1, make_friedman2, make_friedman3, make_sparse_uncorrelated\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples, completeness_contamination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Ograniczenia modeli\n",
    "* modele liniowe mają ograniczenia z powodu klątwy wymiarowości\n",
    "* niewystarczające są __ustalone__ funkcje bazowe\n",
    "  * zły wybór może powodować overfitting  \n",
    "  * trzeba __adaptować__ funkcje do konkretnych danych\n",
    "  * ograniczyć liczbę funkcji bazowych\n",
    "\n",
    "* alternatywne podejście\n",
    "  * ustalić funkcje bazowe z góry - niewielką liczbę, zwykle __jedną__\n",
    "  * niech funkcje __adaptują__ się w trakcie nauki\n",
    "  * model jest zwykle znacznie mniejszy\n",
    "  * funkcja ___likelihood_ nie będzie już wypukła__\n",
    "    * konieczność przeglądnięcia większej liczby modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron biologiczny\n",
    "\n",
    "  <img src=\"../mum_figures/neuron.png\" width=\"100%\"/>\n",
    "  \n",
    "  <img src=\"../mum_figures/synapse.jpg\" width=\"100%\"/>\n",
    "\n",
    "![Potential](file:///Users/igorpodolak/Dropbox/mum_figures/potential.gif)\n",
    "\n",
    "![Awesome cat gif](http://media.giphy.com/media/7z2oyDXIMEs8w/giphy.gif)\n",
    "\n",
    "![Potential](http://github.com/gmum/ml2017/wyklady/mum_figures/potential.gif)\n",
    "\n",
    "<img src=\"../mum_figures/potential.jpg\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieci z propagacją sygnału (feed-forward)\n",
    "<img src=\"../mum_figures/mlp.png\" width=\"100%\"/>\n",
    "* funkcje bazowe modelu\n",
    "  * muszą __zależeć__ od parametrów\n",
    "  * parametry powinny się zmieniać w trakcie uczenia\n",
    "  * współczynniki $w_j$ także są modyfikowalne\n",
    "* $M$ liniowych kombinacji wejść $$v_j=\\sum_i^Dw_{ji}x_i+w_{j0}$$\n",
    "gdzie $M$ jest liczbą __\"neuronów\"__ w pierwszej __warstwie__ modelu\n",
    "  * $w_{j0}$ to __bias__ $j$-tego neuronu\n",
    "  * $v_j$ to __aktywacje__\n",
    "  * teraz użyta jest __funkcja aktywacji__ $\\varphi()$ (różniczkowalna) $$z_j=\\varphi(v_j)$$\n",
    "    * funkcje aktywacji są zwykle funkcjami sigmoidalnymi, np.\n",
    "    $$\\frac{1}{1+\\exp({-v})}, \\tanh(-v)$$\n",
    "  * neurony w warstwach, które nie są ani wejściową ani ukrytą nazywamy __ukrytymi__\n",
    "  \n",
    "  \n",
    "* obliczenia powtarzają się w kolejnych warstwach\n",
    "* końcowa wartość jest jako, dla sieci z jedną warstwą ukrytą\n",
    "$$y_k(x)=\\sigma\\left(\\sum_k^Mw_kj\\sigma\\left(\\sum_i^Dw_{ji}x_i+w_{j0}\\right)+w_{k0}\\right)$$\n",
    "  * jeśli problem regresji, to końcowa funkcja aktywacji jest identycznością\n",
    "  * sieć neuronowa jest __nieliniową__ funkcją wejść $x_i$\n",
    "    * wartość bias włączamy do wektora $w$ __rozszerzając__ wejście o $x_0=1$\n",
    "\n",
    "* sygnał jest przesyłany do przodu, stąd nazwa\n",
    "* sieć wielowarstwowa nazywana __wielowarstwowym perceptronem__\n",
    "  * zaproponowany przez McCullocha i Pittsa __perceptron__ \n",
    "    * wykorzystywał __progową__ funkcję aktywacji (f. Heaviside'a)\n",
    "    * nie zawierał warstw ukrytych\n",
    "\n",
    "* sieć z __liniowymi__ f. aktywacji w w. ukrytych jest redukowalna do sieci __bez__ warstw ukrytych\n",
    "* sieci warstwowe można uogólnić przez dodanie __połączeń skrótowych__\n",
    "\n",
    "* nazewnictwo\n",
    "  * istnieje wiele zwyczajów: sieć na rysunku może być nazywana \n",
    "    * 4-warstwową (bo liczba wszystkich warstw neuronów), \n",
    "    * 3-warstwową (bo liczba warstw adaptowalnych parametrów), \n",
    "    * 2-warstwową (bo liczba warstw ukrytych)\n",
    "  * Bishop preferuje drugie podejście\n",
    "  * jasne będzie \"sieć z dwoma warstwami ukrytymi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieci neuronowe jako uniwersalny aproksymator\n",
    "> Sieć neuronowa z jedną warstwą ukrytą neuronów z nieliniowymi ciągłymi monotonicznie rosnącymi funkcjami aktywacji może dowolnie dokładnie dla $\\epsilon>0$ aproksymować dowolną funkcję ciągła zdefiniowaną na hiperkostce $[0, 1]^m$.\n",
    "\n",
    "> Twierdzenie jest o _istnieniu_ takiej sieci, a nie mówi jak znaleźć odpowiednią liczbę neuronów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja kosztu: regresja\n",
    "* niech przewidywana pojedyncza zmienna $t$ ma rozkład normalny zależny od wejściowego atrybutu $x$ $$p(t\\mid x)=\\mathcal{N}(t\\mid y(x, w), \\sigma)$$\n",
    "gdzie\n",
    "  * $y(x, w)$ jest wyjściem sieci\n",
    "  * $\\sigma$ jest wariancją\n",
    "* niech $X=\\{x_1,x_2,\\dots,x_N\\}$ będzie zbiorem $N$ __niezależnych__ przykładów\n",
    "* dla zbioru wartości spodziewanych (target) $\\{t_1,\\dots,t_N\\}$ można utworzyć funkcję __likelihood__ \n",
    "$$p(t\\mid X,w,\\sigma)=\\prod_{n=1}^Np(t_n\\mid x_n,w,\\sigma)$$\n",
    "\n",
    "* biorąc __ujemny logarytm__ dostajemy __log-likelihood__\n",
    "$$\\frac{1}{2\\sigma}\\sum_{n=1}^N\\left(y(x_n,w)-t_n\\right)^2+\\frac{N}{2}\\ln\\sigma +\\frac{N}{2}\\ln(2\\pi)$$\n",
    "* maksymalizacja likelihood jest równoważna __minimalizacji__ log-likelihood, co jest identyczne z minimalizacją funkcji kosztu\n",
    "$$E(w)=\\frac{1}{2}\\sum_{n=1}^N\\left(y(x_n,w)-t_n\\right)^2$$\n",
    "  * nieliniowość funkcji $y(x,w)$ powoduje, że $E(w)$ __nie jest wypukła__\n",
    "  * funkcja kosztu będzie miała __minima lokalne__\n",
    "  * po znalezieniu wektora $w^\\ast$ można znaleźć $\\sigma$ jako\n",
    "  $$\\sigma=\\frac{1}{N}\\sum_{n=1}^N\\left(y(x_n,w^\\ast)-t_n\\right)^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja kosztu: klasyfikacja\n",
    "* przypadek klasyfikacji binarnej:\n",
    "  * pojedynczy neuron wyjsciowy z $\\sigma()$ __logistyczną__\n",
    "  $$y=\\sigma(v)=\\frac{1}{1+\\exp(-v)}$$\n",
    "  * niech $t=1$ jeśli $x\\in C_1$; $t=2$ jeśli $x\\in C_2$\n",
    "  * niech $y(x,w)$ będzie __prawdopodobieństwem__, że $x\\in C_1$\n",
    "  * $1-y(x,w)$ prawdopodobieństwem, że $x\\in C_2$\n",
    "  \n",
    "* rozkład warunkowy wyjść jest rozkładem Bernoulliego\n",
    "$$p(t\\mid x)=y(x,w)^t\\left(1-y(x,w)\\right)^{1-t}$$\n",
    "\n",
    "* dla zbioru wszystkich przykładów mamy likelihood\n",
    "$$p(t\\mid X,w)=\\prod_ny(x_n,w)^t_n\\left(1-y(x_n,w)\\right)^{1-t_n}$$\n",
    "* znowu biorąc ujemny logarytm dostajemy log-likelihood i funkcję kosztu\n",
    "$$E(w)=-\\sum_{n=1}^N\\left[t_n\\ln\\,y_n+(1-t_n)\\ln\\,(1-y_n)\\right]$$\n",
    "nazywaną __entropią krzyżową__\n",
    "  \n",
    "  * dla klasyfikacji użycie entropii krzyżowej daje lepsze wyniki niż użycie funkcji kwadratowej kosztu\n",
    "* dla problemu wieloklasowego $t\\in\\{C_1,\\dots,C_K\\}$ mamy\n",
    "$$E(w)=-\\sum_{n=1}^N\\sum_{k=1}^K\\left[t_{nk}\\ln\\,y_k(x_n,w)+(1-t_{nk})\\ln\\,(1-y_k(x_n,w))\\right]$$\n",
    "* dla problemów klasyfikacji wieloklasowej mamy\n",
    "  $$y_k(x,w)=p(t_k=1\\mid x)$$\n",
    "  * to skraca wyrażenie entropii krzyżowej do\n",
    "  $$E(w)=-\\sum_{n=1}^N\\sum_{k=1}^Kt_n\\ln\\,y_k(x_n,w)$$\n",
    "  * dla problemu wieloklasowego aktywacją wyjściową jest funkcja softmax\n",
    "  $$y_k(x,w)=\\frac{\\exp(v_k(x,w))}{\\sum_{j=1}^K\\exp(v_j(x,w))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cechy\n",
    "* podstawowa różnica do modeli liniowych: sieć neuronowa __mapuje przestrzeń wejściową do przestrzeni ukrytej gdzie znajduje rozwiązanie__\n",
    "  * __nieliniowe__ rzutowanie do przestrzeni ukrytej zwiększa prawdopodobieństwa liniowej separowalności\n",
    "\n",
    "* aktywacje warstw ukrytych dla danego wejścia nazywamy cechami\n",
    "  * w modelach każda przynależność do klasy jest rozwiązywana jako __całkowicie osobny__ problem\n",
    "  * w sieciach podproblemy przynależności do klas __dzielą cechy__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie sieci warstwowych neuronowych\n",
    "* zadaniem jest minimalizacja $E(w)$\n",
    "* drobna zmiana $w$ powoduje __presunięcie__ się po powierzchni funkcji kosztu\n",
    "  * dualność: każde rozwiązanie odpowiada punktowi w przestrzeni wag\n",
    "* przestrzeń rozwiązań jest zwykle bardzo wysoko wymiarowa\n",
    "  * istnieje wiele punktów o $\\nabla{}E(w)\\simeq0$\n",
    "  * to minima/maksima lokalne/globalne, punkty siodłowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lokalna aproksymacja kwadratowa\n",
    "* rozwinięcie funkcji kosztu w szereg Taylora\n",
    "$$E(w)\\simeq E(w_0)+(w-w_0)g+\\frac{1}{2}(w-w_0)H(w-w_0)$$\n",
    "gdzie \n",
    "$$\\begin{align}\n",
    "g=&\\frac{\\partial E}{\\partial w}\\\\\n",
    "H=&\\frac{\\partial^2 E}{\\partial w_i\\,\\partial w_j}\n",
    "\\end{align}$$\n",
    "\n",
    "* stąd\n",
    "$$\\nabla E\\simeq g+H(w-w_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie gradientowe\n",
    "* uczenie małymi krokami\n",
    "$$w(t+1)=w(t)-\\eta\\nabla E(w(t))$$\n",
    "gdzie $\\eta$ jest __współczynnikiem nauczania__\n",
    "\n",
    "* to metoda __epoka po epoce__ (ang. batch)\n",
    "  * każdy krok poprawy wymaga wielu obliczeń\n",
    "  * każda propagacja wprzód używa starych wag\n",
    "  * wiele przykładów dzieli wspólne cechy, stąd utrata wielu możliwych informacji\n",
    "  * uczenie po epokach może powodować krążenie w przestrzeni rozwiązań\n",
    "  * usprawnienie: metoda gradientów sprzężonych zapewniająca poprawy\n",
    "    * każda poprawa jest ortogonalna do poprzednich\n",
    "    * teoretycznie roziązanie po co najwyżej tylu krokach ile jest wag\n",
    "\n",
    "* epoka __przykład po przykładzie__ (ang. stochastic gradient descent)\n",
    "$$E(w)=\\sum_nE_n(w)$$\n",
    "  * poprawa po każdym przykładzie\n",
    "  $$w(t+1)=w(t)-\\eta\\nabla E_n(w(t))$$\n",
    "  * uwzględnia wiedzę z innych przykładów od razu\n",
    "  * pozwala na łatwiejsze omijania minimów lokalnych\n",
    " \n",
    "* __mini batch__\n",
    "  * uczenie małymi grupami przykładów\n",
    "  * najbardziej efektywne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wsteczna propagacja błędów\n",
    "* sieci przechodziły przez wzloty i upadki\n",
    "* zapowiadano (Minsky i Papert w _Perceptron_) śmierć ze względu na __niemożność__ uczenia sieci z warstwami ukrytymi\n",
    "* problem z liczeniem gradientu dla warstw ukrytych\n",
    "* algorytm wstecznej propagacji zaproponowany przez Werbosa\n",
    "\n",
    "\n",
    "* w trakcie uczenia obliczana jest duża liczba pochodnych funkcji kosztu __względem__ wag\n",
    "* wsteczna propagacja jest __efektywnym__ machenizmem ich obliczania\n",
    "  * pochodne (błędy) są przesyłane wstecz w sieci\n",
    "* drugim elementem jest obliczenie i modyfikacja wartości wag\n",
    "\n",
    "* schemat może być uzyty do szerokiego wachlarza architektur sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pochodne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularyzacja"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
