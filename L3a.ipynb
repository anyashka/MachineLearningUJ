{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> L3a: Regresja liniowa w podejściu Bayesowskim i nie </h1>\n",
    "\n",
    "W ramach dzisiejszych zajęć:\n",
    "\n",
    "1. rozważymy problem uczenia się regresji liniowej w podejściu maximum likelihood (MLE) oraz bayesowskim. \n",
    "\n",
    "2. użyjemy implementacji scikit-learn na tradycyjnym zbiorze cyfr MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Szybkie) przypomnienie\n",
    "\n",
    "Spojrzenie probabilistyczne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=300 src=\"figures/L3/frequentists_vs_bayesians.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przypomnienie\n",
    "\n",
    "Machine learning jako robienie ``push-up`` rozkładu prior\n",
    "\n",
    "* P(w) - prior\n",
    "\n",
    "* P(x | w) - likelihood\n",
    "\n",
    "* P(w | x) - posterior\n",
    "\n",
    "Na obrazku powyzej osoba po lewej liczy P(w=0 | x), co nie zawsze jest przydatne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=400 src=\"figures/L3/pavel.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przyklad\n",
    "\n",
    "Źródło: http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb\n",
    "\n",
    "[Omówienie przykładu na zajęciach]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresja liniowa\n",
    "\n",
    "Źródło: https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/lecture3.pdf\n",
    "\n",
    "Znamy już świetnie rozkład Gaussa. Podobnie jak w naszych eksperymentach z rzutami monetą, podejście maximum likelihood jest relatywnie proste.\n",
    "\n",
    "Załóżmy, że dane pochodzą rozkładu normalnego, czyli podobnie jak w przypadku rzutu monetą zakładamy, że:\n",
    "\n",
    "$$ p(y | x) = N(<x, w>, \\sigma^2) = <x, w> + N(0, \\sigma^2) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje bazowe\n",
    "\n",
    "Nic więcej poza nową reprezentacją.\n",
    "\n",
    " $y(\\mathbf{x},\\mathbf{w}) = \\sum_{j=0}^{M-1} w_j\\phi_j(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{\\phi}(\\mathbf{x})$\n",
    " \n",
    " , gdzie $\\phi(\\mathbf{x}) = \\\\{\\phi_0(\\mathbf{x}),\\phi_1(\\mathbf{x}), \\ldots,\\phi_{M-1}(\\mathbf{x})\\\\}$\n",
    "\n",
    "## Estymator MLE\n",
    "\n",
    "\n",
    "$$ \\theta^* = argmax \\prod_i p(y_i | x_i) $$\n",
    "\n",
    "$\\theta^* = \\left(\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\right)^{-1}\\mathbf{\\Phi}^T\\mathbf{t}$ <br/><br/>\n",
    "\n",
    "[Wyprowadzenie na tablicy wzoru na estymator MLE, https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/lecture3.pdf. Uwaga! To może być na egzaminie]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cwiczenie 1 (3 pkt) - Kernelized Linear Regression\n",
    "\n",
    "Dane pochodzą z rozkładu y_i = m*x_i + b + N(0, 1). \n",
    "\n",
    "Na podstawie 101 przykładów z rozkładu policz estymator MLE następującego modelu:\n",
    "\n",
    "a) $$ p(y | x) = N(<x, w> + b, \\sigma^2) = <x, w> + b + N(0, \\sigma^2) $$. Równoważne $\\phi(x) = [1, x]$ (patrz podpunkt b)\n",
    "\n",
    "b) $$ p(y | x) = N(<\\phi(x), w>, \\sigma^2) = <\\phi(x), w> + N(0, \\sigma^2) $$, gdzie $\\phi(x) = [1, x, x^2, x^3, x^4]$\n",
    "\n",
    "Kod powinien być opakowany w klase ToyLinearRegression z metodami fit oraz predict. ToyLinearRegression powinien przyjmować w konstruktorze funkcję phi, która zamienia x na jego cechy, np phi(x) = [x] oznacza liniowy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Syntetyczny zbiór danych z dużym szumem ale liniowymi danymi\n",
    "np.random.seed(123456789)\n",
    "\n",
    "def y(x,m,b,mu=0,sigma=1.0): \n",
    "    return m*x + b + np.random.normal(mu,sigma,1)[0]\n",
    "\n",
    "N = 101\n",
    "M = 2\n",
    "t = np.empty(N)\n",
    "domain_bound = 1.0/N\n",
    "domain = np.empty(N)\n",
    "\n",
    "for i in range(N): \n",
    "    domain[i] = i*domain_bound\n",
    "    \n",
    "for i in range(N): \n",
    "    t[i] = y(x=domain[i],m=4.89,b=0.57)\n",
    "\n",
    "plt.plot(domain, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_pred = ToyLinearRegression(phi=lambda x: [1, x]).fit(domain, t).w\n",
    "assert np.abs(w_pred - np.array([0.88610652, 4.47519348])).max() < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesowska regresja liniowa\n",
    "\n",
    "Jak zwykle proces modelowania danych zaczynamy od załóżenia czegoś o rozkładzie. \n",
    "\n",
    "Wcześniej, zakładaliśmy że ciąg obserwacji rzutów monetą jest dobrze modelowane ciągiem niezależnych zdarzeń, każde o rozkładzie Bernoulliego. Załóżmy teraz, że obserwujemy ciąg par (x_i, y_i) (patrz rysunek poniżej), oraz że każde y_i można całkiem dobrze przewidzieć z x_i za pomocą wzoru y_i = m*x_i + b + N(0, 1).\n",
    "\n",
    "<img src=\"figures/L2/linear_reg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cwiczenie 2 (4 pkt) - symulacja MAP \n",
    "\n",
    "W ostatnim zadaniu stworzyliśmy klasę ToyLinearRegression. W tym ćwiczeniu masz za zadanie oszacować jak będzie wyglądał posterior dla paru różnych rozkładów prior.\n",
    "\n",
    "Niech prior będzie zdefiniowany poniższą funkcją. Niech $phi(x) = [1, x, x^2]$. Niech w[0]=0.88 na stałe. Niech sigma=1.0\n",
    "\n",
    "a) Narysuj prior w[1] oraz w[2] (np. uzywajac meshgrid) dla C=1 i C=100 w zakresie [0, 10]\n",
    "\n",
    "b) Napisz funkcję o sygnaturze loglikelihood(w, X, Y, phi=lambda x: [1, x], sigma=1.0). \n",
    "\n",
    "c) Narysuj likelihood oraz posterior w[1] oraz w[2] dla C=0.1 i C=100 w zakresie [0, 10].  Dobierz tak sigma aby posterior miał wyraźny kształt \"kulkowaty\" jak na rysunku. \n",
    "\n",
    "d) Wyestymuj MAP oraz MLE dla C=0.1 i C=100. Co zauważyłeś? Z czego to wynika? (napisać)\n",
    "\n",
    "\n",
    "### Hints\n",
    "\n",
    "Hint: estymacja MAP/MLE polega na wybraniu argmaxa logposterior/loglikelihood po meshgrid\n",
    "\n",
    "Hint: w c) nalezy liczyc *log* likelihood i *log* posterior\n",
    "\n",
    "Hint: w d) uwaga, logposterior to nie ject loglikelihood*prior. Zastanowić się :)\n",
    "\n",
    "Hint: funkcje prior należy wywoływać na całej macierzy punktów, nie tylko pojedynczych punktach. Będzie szybciej\n",
    "\n",
    "### Co powinno wyjść\n",
    "\n",
    "Prior: \n",
    "\n",
    "<img width=200 src=\"figures/L3/l3a_2_1.png\">\n",
    "\n",
    "C=0.1, od lewej: prior/loglikelihood/logposterior: \n",
    "\n",
    "<img width=200 src=\"figures/L3/l3a_2_2.png\">\n",
    "\n",
    "C=100, od lewej: prior/loglikelihood/logposterior: \n",
    "\n",
    "<img width=200 src=\"figures/L3/l3a_2_3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prior(w, C, mean=[0.0, 3.0, 0.0]):\n",
    "    assert w.ndim == 2\n",
    "    \n",
    "    sigma = np.zeros(shape=(w.shape[1], w.shape[1]))\n",
    "    sigma[0,0] = 200 # Bias slabo zregularyzowany\n",
    "    \n",
    "    # Mozna ladniej zapisac\n",
    "    for id in range(1, w.shape[1]):\n",
    "        sigma[id, id] = C # Duze C -> malo zregularyzowany\n",
    "        \n",
    "    return scipy.stats.multivariate_normal.pdf(w,mean,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loglikelihood(w, X, Y, phi=lambda x: [1, x], sigma=1.0):\n",
    "    # Twoj kod!\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ewaluacja\n",
    "\n",
    "Cały problem w uczeniu maszynowym polega na tym, że nie znamy danych testujących, tj. danych na których nasz model będzie używany.\n",
    "\n",
    "Na potrzeby tych ćwiczeń załóżmy że nasz model będzie aplikowany na danych pochodzących z tego samego rozkładu co zbiór który dostaliśmy. Aby *zasymulować* tą sytuację, dzielimy zbiór na zbiór trenujący i testujący *zupełnie losowo*.\n",
    "\n",
    "<img src=\"figures/L3/K-fold_cross_validation_EN.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cwiczenie 3 (3 pkt) - MAP oraz co to znaczy dobra regularyzacja?\n",
    "\n",
    "W tym ćwiczeniu Twoim zadaniem jest stworzenie klasy ToyBayesianLinearRegression, która nauczy model bayesowskiej regresji liniowej z wykorzystaniem liczenia bezposrednio wzoru na posterior.\n",
    "\n",
    "Niech $phi(x) = [1, x, x^2]$, niech $sigma=1.0$\n",
    "\n",
    "1. Podziel zbiór domain na zbiór trenujący i testujący uzywajac: ``X_train, X_test, Y_train, Y_test = train_test_split(X,Y, random_state=1)``\n",
    "\n",
    "2. Zmodyfikuj ToyLinearRegression aby używał wzorów na MAP [źródło: Bishop, albo http://kudkudak.github.io/assets/pdf/talks/gp.pdf]\n",
    "\n",
    "3. Policz wynik na zbiorze tesującym modelu nauczonego na zbiorze trenującym z C=0.1 oraz C=100\n",
    "\n",
    "4. Które C jest lepsze i dlaczego? Upewnij się, że Twój argument uwzględnia to jaki jest *prawdziwy* model danych.\n",
    "\n",
    "Hint: Wersja Bayesowska sprowadza się do *bardzo prostej modyfikacji ćw 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Maaala regularyzacja -> to samo co cw. 1. Polecam sprawdzic ze tak wychodzi w cw. 1 przy danym tutaj phi\n",
    "w_pred = ToyBayesianLinearRegression(C=1000000, phi=lambda x: [1, x, x**2]).fit(domain, t).w\n",
    "assert np.abs(w_pred - np.array([1.37, 1.475, 3.023])).max() < 0.05\n",
    "\n",
    "# 2. Duza regularyzacja -> zabija obydwa\n",
    "w_pred = ToyBayesianLinearRegression(C=0.01, phi=lambda x: [1, x, x**2]).fit(domain, t).w\n",
    "assert np.abs(w_pred - np.array([2.832, 0.3217, 0.33])).max() < 0.05\n",
    "\n",
    "# 3. Mala regularyzacja\n",
    "w_pred = ToyBayesianLinearRegression(C=1, phi=lambda x: [1, x, x**2]).fit(domain, t).w\n",
    "assert np.abs(w_pred - np.array([1.376, 1.944, 2.322])).max() < 0.05"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
